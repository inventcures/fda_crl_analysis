{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA CRL Exploratory Analysis\n",
    "\n",
    "Interactive exploration of FDA Complete Response Letter patterns.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Our modules\n",
    "from data_acquisition import CRLDataAcquisition\n",
    "from pdf_parser import CRLParser\n",
    "from analysis import CRLAnalyzer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Parse CRL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CRL data (run once)\n",
    "acq = CRLDataAcquisition(data_dir='../data')\n",
    "results = acq.download_and_extract_all(force=False)\n",
    "manifest = acq.create_manifest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse PDFs (run once, takes time)\n",
    "parser = CRLParser(extraction_method='auto')\n",
    "\n",
    "# Parse approved CRLs\n",
    "approved_docs = parser.parse_directory(\n",
    "    Path('../data/raw/approved_crls'), \n",
    "    'approved',\n",
    "    limit=50  # Start small for testing\n",
    ")\n",
    "\n",
    "# Parse unapproved CRLs\n",
    "unapproved_docs = parser.parse_directory(\n",
    "    Path('../data/raw/unapproved_crls'),\n",
    "    'unapproved',\n",
    "    limit=50\n",
    ")\n",
    "\n",
    "all_docs = approved_docs + unapproved_docs\n",
    "print(f\"Parsed {len(all_docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parsed data\n",
    "parser.save_parsed_data(all_docs, Path('../data/processed/parsed_crls.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parsed data\n",
    "analyzer = CRLAnalyzer(data_path=Path('../data/processed/parsed_crls.json'))\n",
    "df = analyzer.df\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\n=== Approval Status ===\")\n",
    "print(df['approval_status'].value_counts())\n",
    "\n",
    "print(\"\\n=== Application Types ===\")\n",
    "print(df['application_type'].value_counts())\n",
    "\n",
    "print(\"\\n=== Page Count Stats ===\")\n",
    "print(df['page_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deficiency Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deficiency frequency\n",
    "freq = analyzer.deficiency_frequency_analysis()\n",
    "\n",
    "print(\"Overall deficiency counts:\")\n",
    "for cat, count in sorted(freq['overall'].items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize deficiency frequency\n",
    "analyzer.plot_deficiency_frequency(save_path='../outputs/deficiency_frequency.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence heatmap\n",
    "analyzer.plot_cooccurrence_heatmap(save_path='../outputs/cooccurrence.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rescue Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rescue rates\n",
    "rescue_rates = analyzer.calculate_rescue_rates()\n",
    "rescue_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rescue rates\n",
    "analyzer.plot_rescue_rates(save_path='../outputs/rescue_rates.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier\n",
    "classifier_results = analyzer.build_approval_classifier()\n",
    "\n",
    "# Print performance\n",
    "for name, results in classifier_results['models'].items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Test Accuracy: {results['test_accuracy']:.3f}\")\n",
    "    print(f\"  CV Mean: {results['cv_mean']:.3f} (Â±{results['cv_std']:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "analyzer.plot_feature_importance(\n",
    "    classifier_results, \n",
    "    model_name='Random Forest',\n",
    "    save_path='../outputs/feature_importance.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "analyzer.plot_roc_curves(classifier_results, save_path='../outputs/roc_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run statistical tests\n",
    "stats_results = analyzer.statistical_tests()\n",
    "\n",
    "# Print significant features\n",
    "print(\"Statistically significant features (p < 0.05):\")\n",
    "for feature, result in stats_results.items():\n",
    "    if result.get('significant', False):\n",
    "        print(f\"  {feature}: p={result['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize statistical comparison\n",
    "analyzer.plot_statistical_comparison(save_path='../outputs/statistical_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Dive: Sample CRL Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a specific CRL\n",
    "sample = df.iloc[0]\n",
    "\n",
    "print(f\"Drug: {sample.get('drug_name', 'Unknown')}\")\n",
    "print(f\"Application: {sample.get('application_type', 'Unknown')} {sample.get('application_number', 'Unknown')}\")\n",
    "print(f\"Status: {sample['approval_status']}\")\n",
    "print(f\"\\nDeficiency Categories: {sample.get('deficiency_categories', [])}\")\n",
    "print(f\"\\nSafety Concerns: {sample.get('has_safety_concerns', False)}\")\n",
    "print(f\"Efficacy Concerns: {sample.get('has_efficacy_concerns', False)}\")\n",
    "print(f\"CMC Issues: {sample.get('has_cmc_issues', False)}\")\n",
    "print(f\"Requests New Trial: {sample.get('requests_new_trial', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Analysis: Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your custom analysis here\n",
    "# Example: Filter by specific criteria\n",
    "\n",
    "# CRLs with both safety and efficacy concerns\n",
    "both_concerns = df[\n",
    "    (df['has_safety_concerns'] == True) & \n",
    "    (df['has_efficacy_concerns'] == True)\n",
    "]\n",
    "\n",
    "print(f\"CRLs with both safety AND efficacy concerns: {len(both_concerns)}\")\n",
    "print(f\"Approval rate: {(both_concerns['approval_status'] == 'approved').mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "summary = analyzer.generate_full_analysis(output_dir=Path('../outputs'))\n",
    "print(\"\\nAnalysis complete! Check ../outputs/ for all visualizations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
