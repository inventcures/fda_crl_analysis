# FDA Complete Response Letter (CRL) Analysis Pipeline

[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/inventcures/fda_crl_analysis)

> **Extract insights from FDA rejection patterns to accelerate drug development**

This toolkit downloads, parses, and analyzes FDA Complete Response Letters from the OpenFDA database to reveal patterns in regulatory decision-making that can help drug developers avoid common pitfalls and increase approval probability.

## ğŸ† Top 3 High-Impact Analyses

Based on impact to the drug discovery community and ability to reveal FDA decision-making patterns:

---

### #1: Approved vs. Unapproved Discriminative Feature Analysis

**Impact: â­â­â­â­â­ (Highest)**

**Why this matters:**
- This is the holy grail question: *What separates drugs that recover from CRLs vs. those that don't?*
- Directly actionable for drug developers designing trials and regulatory strategies
- Can reveal "hidden rules" in FDA decision-making that aren't in official guidance

**What it reveals:**
- Which deficiency types are "death sentences" vs. "speed bumps"
- Whether certain combinations of issues are particularly problematic
- If application type (NDA vs. BLA) or therapeutic area influences recovery
- The threshold of deficiency count/severity that predicts failure

**Key outputs:**
- Feature importance rankings (Random Forest, XGBoost)
- ROC curves showing prediction accuracy
- SHAP values for interpretability
- Risk score calculator for new applications

**Example insight:** *"CRLs citing both safety concerns AND requests for new clinical trials have only 12% rescue rate vs. 78% for manufacturing-only issues"*

```python
from src.analysis import CRLAnalyzer

analyzer = CRLAnalyzer(data_path="data/processed/parsed_crls.json")
results = analyzer.build_approval_classifier()
analyzer.plot_feature_importance(results, save_path="outputs/feature_importance.png")
```

---

### #2: Rescue Rate by Deficiency Category

**Impact: â­â­â­â­â­ (Highest)**

**Why this matters:**
- Quantifies the "recoverability" of each deficiency type
- Essential for portfolio risk management and investment decisions
- Helps sponsors prioritize resources on fixable vs. terminal issues

**What it reveals:**
- Manufacturing issues â†’ High rescue rate (often fixable)
- Fundamental efficacy failures â†’ Low rescue rate (rarely recoverable)
- Which combination patterns have synergistic negative effects
- Time-to-resolution by deficiency type

**Key outputs:**
- Rescue rate bar chart by category
- Kaplan-Meier survival curves for time-to-approval
- Sankey diagram showing CRL â†’ outcome flow
- Risk matrix (deficiency type Ã— severity â†’ rescue probability)

**Example insight:** *"CMC/manufacturing deficiencies have 87% rescue rate with median 8-month resolution, while 'failed to demonstrate efficacy' has 23% rescue rate with median 2.5-year resolution"*

```python
rescue_rates = analyzer.calculate_rescue_rates()
analyzer.plot_rescue_rates(save_path="outputs/rescue_rates.png")
```

---

### #3: LLM-Powered Deficiency Extraction & Severity Scoring

**Impact: â­â­â­â­ (Very High)**

**Why this matters:**
- CRLs contain nuanced, unstructured language that keyword matching misses
- LLMs can extract FDA's implicit severity signals from phrasing
- Enables semantic search across the corpus ("find all CRLs with hepatotoxicity concerns")
- Creates structured database from free-text for downstream ML

**What it reveals:**
- Granular deficiency taxonomy beyond top-level categories
- FDA's "tone" and implicit severity (confident rejection vs. request for clarification)
- Specific remediation recommendations and their implied complexity
- Patterns in FDA reviewer language over time

**Key outputs:**
- Structured deficiency database with severity scores
- Semantic embeddings for similarity search
- Automatic classification of resubmission class (I vs. II)
- Remediation complexity estimates

**Example insight:** *"CRLs using language like 'cannot determine' have 34% lower rescue rate than those with 'additional data needed', suggesting FDA confidence level is predictive"*

```python
from src.llm_analysis import LLMAnalyzer

llm = LLMAnalyzer(api_key="your-key")
extractions = llm.analyze_batch(documents, output_path="outputs/llm_extractions.json")

# Comparative analysis
comparison = llm.comparative_analysis(approved_extractions, unapproved_extractions)
```

---

## Quick Start

### Installation

```bash
# Clone/download project
cd fda_crl_analysis

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Run Full Pipeline

```bash
# Download, parse, analyze, and run language analysis
python main.py --download --parse --analyze --language

# With LLM analysis (requires API key)
export ANTHROPIC_API_KEY="your-key"
python main.py --download --parse --llm-analyze --analyze --language
```

### Run Individual Stages

```bash
# Download only
python main.py --download

# Parse PDFs (requires downloaded data)
python main.py --parse --limit 50  # Test with 50 docs

# Parse with raw text (needed for language analysis)
python main.py --parse --include-raw-text

# Analysis only (requires parsed data)
python main.py --analyze --parsed-data data/processed/parsed_crls.json

# Language analysis only
python main.py --language --parsed-data data/processed/parsed_crls.json
```

## Project Structure

```
fda_crl_analysis/
â”œâ”€â”€ main.py                 # Pipeline orchestrator
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ CLAUDE.md              # Claude Code implementation guide
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_acquisition.py # Download from OpenFDA
â”‚   â”œâ”€â”€ pdf_parser.py       # PDF text extraction & parsing
â”‚   â”œâ”€â”€ llm_analysis.py     # Claude-based deep analysis
â”‚   â”œâ”€â”€ analysis.py         # Statistical analysis & visualization
â”‚   â””â”€â”€ language_analysis.py # NLP, sentiment & latent space viz
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_embeddings.py # Generate vector embeddings for search
â”‚   â””â”€â”€ generate_highlights.py # Generate PDF highlight annotations
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ approved_crls/  # Downloaded approved CRL PDFs
â”‚   â”‚   â””â”€â”€ unapproved_crls/ # Downloaded unapproved CRL PDFs
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ parsed_crls.json # Structured extracted data
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ deficiency_frequency.png
â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”œâ”€â”€ roc_curves.png
â”‚   â”œâ”€â”€ rescue_rates.png
â”‚   â”œâ”€â”€ cooccurrence_heatmap.png
â”‚   â”œâ”€â”€ statistical_comparison.png
â”‚   â”œâ”€â”€ analysis_summary.json
â”‚   â””â”€â”€ language/           # Language analysis outputs
â”‚       â”œâ”€â”€ wordcloud_comparison.png
â”‚       â”œâ”€â”€ wordcloud_severity.png
â”‚       â”œâ”€â”€ tsne_embeddings.png
â”‚       â”œâ”€â”€ umap_embeddings.png
â”‚       â”œâ”€â”€ cluster_analysis.png
â”‚       â”œâ”€â”€ topic_model.png
â”‚       â””â”€â”€ severity_landscape.png
â”œâ”€â”€ website/                # Interactive Next.js website
â”‚   â”œâ”€â”€ app/               # Next.js app pages
â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”œâ”€â”€ lib/               # Search & embedding logic
â”‚   â””â”€â”€ public/data/       # Static data & embeddings
â””â”€â”€ notebooks/
    â””â”€â”€ exploratory_analysis.ipynb
```

## All 10 Analysis Ideas

| # | Analysis | Module | Impact |
|---|----------|--------|--------|
| 1 | **Discriminative Features (Approved vs. Unapproved)** | `analysis.py` | â­â­â­â­â­ |
| 2 | **Rescue Rate by Deficiency Category** | `analysis.py` | â­â­â­â­â­ |
| 3 | **LLM Severity & Taxonomy Extraction** | `llm_analysis.py` | â­â­â­â­ |
| 4 | Deficiency Co-occurrence Patterns | `analysis.py` | â­â­â­â­ |
| 5 | Time-to-Resolution Analysis | `analysis.py` | â­â­â­â­ |
| 6 | Therapeutic Area Risk Profiling | `analysis.py` | â­â­â­ |
| 7 | Manufacturing Facility Network Analysis | TBD | â­â­â­ |
| 8 | Sponsor Communication Gap Analysis | TBD | â­â­â­ |
| 9 | Temporal Trends & Policy Impact | `analysis.py` | â­â­â­ |
| 10 | Predictive "Red Flag" Tool | `analysis.py` | â­â­â­â­ |

## Key Visualizations

### Statistical Analysis (analysis.py)
1. **Feature Importance** - Which CRL characteristics predict approval
2. **ROC Curves** - Model performance comparison
3. **Rescue Rate Chart** - Recovery probability by deficiency type
4. **Co-occurrence Heatmap** - Deficiency category relationships
5. **Statistical Comparison** - Distributions with significance tests

### Language & Sentiment Analysis (language_analysis.py)
6. **Comparative Word Clouds** - Approved vs. unapproved language
7. **Severity-Colored Word Cloud** - Terms colored by FDA severity score
8. **N-gram Comparison** - Top bigrams/trigrams by outcome
9. **Severity Distribution** - Histogram + boxplot by approval status
10. **Action Type Radar** - FDA requested actions visualization
11. **Sentiment Trajectory** - Sentiment flow through a document
12. **t-SNE Embeddings** - Documents in latent space (by approval)
13. **UMAP Embeddings** - Alternative dimensionality reduction
14. **Cluster Analysis** - K-means with topic labels
15. **LDA Topic Model** - Topic-word distributions
16. **Severity Landscape** - Latent space colored by severity

## API Reference

### CRLAnalyzer

```python
from src.analysis import CRLAnalyzer

analyzer = CRLAnalyzer(data_path="path/to/parsed_crls.json")

# Run all analyses
summary = analyzer.generate_full_analysis(output_dir="outputs/")

# Individual analyses
freq = analyzer.deficiency_frequency_analysis()
rates = analyzer.calculate_rescue_rates()
classifier = analyzer.build_approval_classifier()
stats = analyzer.statistical_tests()
```

### LLMAnalyzer

```python
from src.llm_analysis import LLMAnalyzer

llm = LLMAnalyzer(api_key="sk-ant-...", model="claude-sonnet-4-20250514")

# Single document
extraction = llm.analyze_single_crl(crl_text, file_hash)

# Batch processing
extractions = llm.analyze_batch(documents, output_path="extractions.json")

# Comparative analysis
comparison = llm.comparative_analysis(approved_list, unapproved_list)
```

### Language Analysis

```python
from src.language_analysis import (
    CRLLanguageAnalysisSuite,
    FDASentimentAnalyzer,
    CRLTextVisualizer,
    CRLLatentSpaceVisualizer
)

# Full analysis suite
suite = CRLLanguageAnalysisSuite()
results = suite.run_full_analysis(documents, output_dir="outputs/language")

# Individual sentiment analysis
analyzer = FDASentimentAnalyzer()
severity = analyzer.calculate_severity_score(text)
certainty = analyzer.calculate_certainty_score(text)
actions = analyzer.extract_action_types(text)

# Text visualizations
text_viz = CRLTextVisualizer()
text_viz.plot_comparative_wordcloud(approved_texts, unapproved_texts, save_path="wordcloud.png")
text_viz.plot_severity_distribution(documents, save_path="severity.png")
text_viz.plot_action_type_radar(documents, save_path="radar.png")

# Latent space visualizations
latent_viz = CRLLatentSpaceVisualizer()
latent_viz.plot_tsne_embeddings(documents, save_path="tsne.png")
latent_viz.plot_umap_embeddings(documents, save_path="umap.png")
latent_viz.plot_cluster_analysis(documents, n_clusters=5, save_path="clusters.png")
latent_viz.plot_topic_model(documents, n_topics=5, save_path="topics.png")
```

## Data Sources

- **Approved CRLs**: https://download.open.fda.gov/approved_CRLs.zip
- **Unapproved CRLs**: https://download.open.fda.gov/unapproved_CRLs.zip
- **CRL Search Table**: https://open.fda.gov/crltable/
- **OpenFDA API**: https://api.fda.gov/other/crl.json

## Limitations

1. **Redactions**: CRLs are heavily redacted for trade secrets/CCI, limiting some analyses
2. **Sample Size**: ~200 approved + ~89 unapproved CRLs may limit statistical power
3. **Selection Bias**: Unapproved CRLs are recent (2024-2025); approved span 2020-2024
4. **OCR Quality**: Some older PDFs may have extraction errors
5. **Missing Context**: CRLs don't include sponsor responses or negotiation history

## Interactive Website

The project includes a fully interactive website for exploring CRL data:

**Live Demo**: [https://fda-crl-analysis.vercel.app](https://fda-crl-analysis.vercel.app)

### Website Features

- **Hybrid Search**: BM25 + Vector semantic search across all 297 CRLs
  - Keyword mode for exact matches (drug names, application numbers)
  - Semantic mode for conceptual queries ("manufacturing issues", "safety concerns")
  - Hybrid mode combines both using Reciprocal Rank Fusion
- **Inline PDF Viewer**: View CRLs with highlight annotations
- **Interactive Dashboards**: Overview, deficiencies, language analysis, predictive models
- **Fully Offline**: Uses transformers.js for client-side embeddings (no API calls)

### Running Locally

```bash
cd website
npm install
npm run dev
# Open http://localhost:3000
```

See `website/README.md` for detailed documentation.

## Contributing

Areas for contribution:
- Additional visualization types (Sankey, network graphs)
- Time series analysis for temporal trends
- Integration with Drugs@FDA for approval dates
- Additional ML models (neural networks, survival analysis)

## License

MIT License - See LICENSE file

## Citation

If you use this toolkit in research, please cite:

```
FDA CRL Analysis Toolkit (2025)
https://github.com/your-repo/fda-crl-analysis
```

## References

1. FDA Press Release: "FDA Embraces Radical Transparency by Publishing Complete Response Letters" (July 2025)
2. Lurie et al. (2015) BMJ Analysis of CRL Disclosure Gaps
3. OpenFDA Documentation: https://open.fda.gov/apis/transparency/completeresponseletters/
